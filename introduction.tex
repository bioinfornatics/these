\begin{refsegment}
\chapter*{De l'information au savoir}
\markboth{Introduction --- De l'information au savoir}{Introduction --- De l'information au savoir}
\addcontentsline{toc}{chapter}{\color{MidnightBlue}\large\textsc\bfseries{Introduction --- De l'information au savoir}}
Avec l'avènement des outils informatiques, la quantité d'information publiée ne cesse d'augmenter. Cette abondance de données disponibles rend plus difficile la gestion de l'information. Une des premières personnes qui a évoqué cette problématique d'explosion de l'information, est Frank Fremont-Smith directeur de l'institut Américain des Sciences Biologiques,  en \citeyear{fremont61}  \cite{fremont61}. Cette problématique est toujours d'actualité. Par exemple depuis 2012, chaque année, plus de 2,8 millions de documents scientifiques sont publiés  \cite{oecd2016} . Face à cette arrivée massive de savoir, la vérification et le croisement de toutes les publications ne sont plus possibles.

De plus, une partie non négligeable de ces documents scientifiques traite de thématiques impliquant un très grand nombre de données. Ainsi, à ce nombre conséquent de publications, s'ajoute une quantité de données importantes et très hétérogènes.

Au regard de cette problématique, comment mettre en place une démarche scientifique pour vérifier nos savoirs ?

Une méthode utilisée jusqu'alors implique la vérification des théories par la mise en place d'expériences répétées. Cette méthodologie basée sur l'expérimentation permet de vérifier une hypothèse par des observations. L'expérimentation doit être menée de manière à ce que les conséquences étudiées soient liées de façon certaine à leur cause.

Il est important de rappeler que cette méthodologie a joué et joue toujours un rôle important dans les découvertes scientifiques. Cette méthode passe dans un premier temps par la formulation d'une hypothèse. Les différents acteurs de l'expérience sont déterminés. Puis dans un second temps, on établit le plan de l'expérience, pour terminer enfin sur l'évaluation des résultats obtenus. Afin de conforter les résultats, l'expérience est répétée. Ce processus permet d'apporter les éléments de confiance et d'impartialité nécessaires pour vérifier une théorie. Toutefois la méthodologie expérimentale s'avère souvent plus longue et plus onéreuse que les méthodes basées sur des prédictions \textit{in silico}. 

\note{Dès l'antiquité, Aristote (384 - 322 av. J.-C.) décrit la nécessité d'expliquer les causes par l'utilisation de conséquences liées et avérées. Mais la première personne reconnue pour avoir utilisée cette méthodologie est Alhazen (965 - 1039). A travers son traité \citetitle{Alhazen1572}, il montre, par des méthodes expérimentales que la lumière voyage en ligne droite. La découverte de cette loi physique est exceptionnelle pour l'époque.}

Les méthodes basées sur le calcul informatique prennent de plus en plus d'importance. Elles sont facilement réutilisables, rapides et capables de traiter un très grand nombre d'informations. Ces méthodes sont développées en utilisant un jeu d'information déterminé au départ. Une fois que les résultats de la méthode sont validés, nous pouvons rechercher un algorithme capable de généraliser et d'apporter une solution globale au problème. Rien ne garantit que les informations fournies par la suite respectent le cadre théorique posé par l'algorithme. C'est pourquoi les résultats obtenus sont considérés comme des prédictions. Ces résultats n'ont pas le même degré de “certification” que ceux obtenus par une observation empirique.

En conséquence, tant les prédictions informatiques que les hypothèses émises par l'Homme devraient, dans l'idéal, être vérifiées par une approche expérimentale. Afin de minimiser le nombre d'expérimentations, est-il intéressant de valider ou non des théories par comparaison à ce que l'on s'attend à obtenir vis-à-vis de ce que l'on prédit ?

Avec l'essor de ces nouvelles technologies numériques, la barrière entre information et savoir devient de plus en plus floue.

\note{
    L'origine du mot science, vient du latin \textit{"scientia"} désignant le savoir.
    
    La connaissance est propre à une personne. À contrario le savoir est transmissible.
    
    De nombreuses langues, comme l'anglais, ne possèdent pas de mot pour différencier savoir et connaissance. En effet, dans les deux cas on utilise \textit{"knowledge"}. Afin de marquer la nuance, on retrouve l'expression \textit{"certified knowledge"} pour le savoir et \textit{"learning by doing"} pour la connaissance.
}


Est-il possible de réunir les observations issues de méthodologies \textit{in silico} et celles issues de l'expérimentation en laboratoire autour d'un modèle théorique ?

Cette question nous amène à réfléchir à la représentation puis la validation des théories. En effet, comment évaluer des théories partiellement observées ? Mais également comment gérer les observations contradictoires ?

Apporter une méthode à cette vaste problématique permettrait de valider nos savoirs, d'identifier les concepts non observés et de caractériser les concepts contradictoires. Représenter ce savoir c'est également faciliter les échanges entre les différents domaines scientifiques.

\citation{La séparation des savoirs, la spécialisation en domaine isolé nuit considérablement au développement de la recherche.}{Jacques Le Goff}[Le Monde de l'éducation - mai 2000]

Ce constat se vérifie en biologie notamment avec l'avènement des séquenceurs de nouvelle génération. Le séquençage des organismes est devenu peu couteux et rapide. Ainsi la communauté scientifique a initié de vastes projets de séquençage de génomes qui visent à déterminer la séquence d' \gls{ADN} des organismes . Elle peut être composée de quelques milliers à plusieurs centaines de millions de paires de bases nucléiques. Selon l'ordonnancement des bases, des régions appelées gènes, confèrent des fonctionnalités à l'organisme. Ainsi, l'\gls{ADN} est un point de départ pour l'étude et la compréhension des fonctionnalités inscrites dans le vivant.


\begin{shadedfigure}
    \centering
    \includegraphics{img/simple_annotation_process.pdf}
    \caption{Vue globale du gène à l'annotation.}
    \label{fig:glob_annotation}
\end{shadedfigure}

Des outils bio-informatiques analysent ces séquences afin de prédire les régions géniques et leurs fonctions dans l'organisme (voir \cref{fig:glob_annotation}). Selon les organismes, le nombre de gènes varie de quelques centaines à plusieurs dizaines de milliers de gènes. Ainsi, parmi le million voire milliard de paires de bases d'\gls{ADN}, il faut identifier tous les gènes. Par conséquent, l'expertise humaine d'un génome est un défi en soi. Ces recherches se sont intensifiées et complexifiées, notamment avec la mise en place du projet d'étude de 100 000 génomes d'organismes pathogènes \cite{100kfoodborne}, ou encore l'analyse des éco-systèmes poly-microbiens présents sur l'Homme \cite{hmp}.

Pour se faire, des outils bio-informatiques ont automatisé le traitement de l'information issue des séquenceurs afin de traiter un nombre d'organismes toujours plus grand. Ces outils, alimentés continuellement en nouveaux génomes, ont amplifié le déluge d'information. Dans le domaine de l'annotation fonctionnelle (i.e. la prédiction de la fonction des gènes et des protéines), moins d'un pour cent des données ont pu être vérifiées (au regard des statistiques publiées par UniProt et SwissProt en 2017 \parencites{uniprot_stat}{expasy_stat} ). Ce fossé entre information de confiance et prédiction s'accélère car il est plus rapide de produire de l'information que de la vérifier.

De plus, les prédicteurs automatiques de la fonction biologique des gènes ne sont pas fiables. En effet, 30\% des annotations fonctionnelles seraient incorrectes, voire 80\% dans certaines familles de protéines \parencites{devos2001intrinsic}{schnoes2009annotation}. Ces séquences incorrectement annotées sont ensuite propagées dans les bases de connaissances.

Une des méthodes d'assignation de fonction de gènes consiste à inférer une annotation provenant d'un gène connu à toutes les séquences similaires. En effet, il est supposé dans l’évolution des organismes que les produits des gènes sont sous pression de sélection afin de préserver leur fonction. L'ensemble des fonctions d'un organisme permet à ce dernier d'être adapté à son environnement. Une modification d'un acide aminé impliqué dans la fonction d’une protéine peut potentiellement entraîner la perte de l'activité biologique de la protéine et par conséquent mettre en péril les capacités de survit de l'organisme. Ces régions d'acides aminés devraient faiblement variées d'un point de vue physico-chimique. Le transfert, de proche en proche des fonctions biologiques tend à surestimer les propriétés biologiques d'une séquence car les séquences annotées de façon erronée se retrouvent parmi les autres et sont potentiellement utilisées, afin de propager leur fonction à un autre gène considéré similaire, détériorant un peu plus la qualité des bases de données.

L'objectif de l'annotation des fonctions géniques est de fournir un catalogue des capacités moléculaires et/ou biochimiques dont est pourvu un organisme. Ce catalogue permet de mieux comprendre le vivant. Cependant, le processus d'annotation produit et amplifie l'assignation de fonctions erronées à de nouveaux gènes et entraîne l'incapacité à utiliser ces prédictions sans prendre un risque. Le catalogue de fonctions géniques est en effet utilisé par la suite dans de nombreux domaines, comme la biochimie, la biologie des systèmes avec l’étude des voies métaboliques et des réseaux de régulation, l'étude des écosystèmes etc. Cette problématique impacte notre compréhension du vivant et notre capacité à l'étudier, remettant en cause tout le processus d'annotation des gènes utilisé jusqu'alors.

Face à cette problématique, des approches variées ont été développées. On distingue les systèmes d'annotation automatique à base de règles, reprenant le raisonnement appliqué par les bio-curateurs, comme le projet \gls{HAMAP} \cite{lima2009hamap}. Ces règles sont généralement basées sur la séquence génomique et la taxonomie de l'organisme. Elles peuvent être créées par un bio-curateur ou par des outils d'apprentissage \cite{uniprot2011ongoing}.

On retrouve également les systèmes de reconstruction des voies métaboliques \cite{karpe2011pathway}. Ces méthodes utilisent des génomes complétement séquencés et bien annotés pour décrire des enchaînements de réactions amenant à des composés d'intérêt biologique.  Cette succession de réactions met en lumière des chemins à travers le réseau de réactions\footnote{D'où le terme anglais "pathway" pour décrire un chemin qui amène à un objectif biologique} qui sont appelés voies métaboliques. Ces réseaux de réactions forment un graphe de connaissance, spécifique de l'organisme. Toutefois, certaines réactions vont apparaître manquantes au regard des activités enzymatiques prédites à partir du génome de l'organisme. Le bio-curateur doit ainsi rechercher dans le génome des gènes candidats pour compléter les étapes d'une voie métabolique. Il est également en mesure d'estimer un degré de complétion des annotations fonctionnelles au regards de ses voies métaboliques prédites dans l'organisme.

Ce travail de curation des annotations par un bio-curateur est nécessaire mais reste une tâche fastidieuse, laborieuse et source d’erreurs. Il apparaît nécessaire de fournir un assistant à la curation de fonctions biologiques des gènes. Une méthode prenant en compte les prédictions \textit{in silico} de fonctions, mais également, le contexte biologique de ces fonctions, pourrait, à partir de règles se rapprochant du raisonnement humain, aider à l’expertise. De plus, elle permettrait de suggérer des annotations fonctionnelles ne pouvant pas être détectées par les prédicteurs usuels. 

\section*{La démarche suivie dans cette thèse}

Mon travail de recherche s'est ouvert à de nombreuses disciplines afin d'apporter une méthode d'expertise des observations vis-à-vis du savoir en biologie. En effet, la complexité du problème nécessite la mise en œuvre de concepts provenant de la logique, la représentation des connaissances, la théorie des graphes, la bio-informatique et le métabolisme. Ainsi, j'ai étudié ces différents domaines, décrypté le jargon, recherché les méthodes semblant avoir une application pour l'expertise de l'annotation fonctionnelle des génomes . Puis, j'ai combiné ces différents concepts dans le but d'obtenir une méthode fiable et rapide.

Certaines voies furent des impasses, d'autres n'avaient pas encore de solution. À travers ces quelques chapitres, je vous livre les notions et mon expérience sur ces différents domaines.

Cette thèse débute avec une problématique biologique \textit{"Comment guider et faciliter le travail des bio-curateurs lors des étapes de l'annotation fonctionnelle ?"}. Pour y répondre, nous avons souhaité reprendre un prototype de vérification de la cohérence globale de l'annotation développée dans l'équipe \texttt{HELIX} de l'Institut National de Recherche en Informatique et en Automatique dirigée par \textit{Alain Viari}. Leur projet a abouti à un raisonneur nommé \gls{HERBS}. Cet outil permet de discriminer les concepts biologiques attendus et correctement prédits des autres concepts. Mon travail de recherche a débuté par le souhait d'étendre les fonctionnalités de \gls{HERBS} afin de prendre en compte l'incertitude et la contradiction. C'est-à-dire d'avoir un raisonneur capable d'utiliser des observations évoquant l'affirmation et l'infirmation mais également le fait de ne pas savoir ou d'avoir des points de vue divergents. Au départ, nous pensions que le travail consisterait à récupérer les méthodes logiques, à les adapter à la biologie, puis mettre à jour l'outil. Or à aucun moment, nous nous doutions que certaines problématiques de la logique étaient toujours ouvertes. En effet, la logique a ses limites, notamment lorsque l'on travaille avec des concepts dont certains peuvent prendre des états ni-vrai-ni-faux ou encore, lorsque l'on souhaite raisonner sur des ensembles d'ensembles. Comme je l'ai compris plus tard, le monde de la logique a été rythmé par des faits marquants comme le paradoxe de \textit{Russell}. Tel un historien, je me suis retrouvé dans les grandes problématiques de la logique moderne. Ainsi j'ai suivi les traces de \textit{Platon} avec les bases de la logique classique, de \textit{Bertrand Russell} avec les ensembles, de \textit{Jan Łukasiewicz} avec le principe de tiers exclu et de \textit{Nuel Belnap} avec la logique à quatre valeurs.

\note{Le paradoxe de Russell démontre que si un ensemble est membre de lui-même, alors par définition il ne peut être un membre de lui-même. Mais s'il n'est pas un membre de lui-même, alors il est un membre de lui-même.
    
    Le paradoxe du barbier image une telle situation. Imaginez, le conseil municipal d'un village vote un arrêté municipal qui enjoint à son barbier (masculin) de raser tous les habitants masculins du village qui ne se rasent pas eux-mêmes et seulement ceux-ci.
    
    Le barbier, qui est bien un habitant du village, n'a pas pu respecter cette règle car :
    \begin{itemize}
        \item S'il se rase lui-même, il enfreint la règle, car le barbier ne peut raser que les hommes qui ne se rasent pas eux-mêmes ;
        \item S'il ne se rase pas lui-même - qu'il se fasse raser ou qu'il conserve la barbe - il est en tort également, car il a la charge de raser les hommes qui ne se rasent pas eux-mêmes.
    \end{itemize}
}

D'autre part, il est apparu très tôt la nécessité de représenter le savoir dans un modèle générique. Un tel modèle permet d'intégrer le savoir provenant d'un large éventail d'entrepôts de données. Cette recherche commença par les travaux de \textit{John F. Sowa} sur la représentation des connaissances ( \citeyear{sowa92,sowa99}). Pour cela, un travail de structuration des concepts et de classification de leurs relations a été mené. Ce travail m'a conduit à étudier la logique de description \cite{baader2003description}. Concrètement, ce domaine de recherche permet de faire le lien entre l'intelligence artificielle et la représentation du savoir. Les études menées sur cette représentation du savoir est un sujet très dynamique, notamment avec l'essor du \textit{Web Semantic}. Cette thématique se consacre à l'étude de la nature des concepts, de leurs relations, mais également de leur existence définissant par la même l'ontologie.

Dans l'objectif de cartographier notre savoir en Biologie, un consortium international s'est créé : \textit{\gls{GO}} (voir G. O. Consortium
et al \citeyear{go2001,go2004}). Les concepts sont classifiés parmi trois catégories distinctes (i) les fonctions moléculaires, (ii) les processus biologiques et (iii) les composants cellulaires. Les concepts sont appelés des GO termes. Les relations entre les termes possèdent des étiquettes pour exprimer les notions de composition et de type \cref{fig:go_relation}. Ces descriptions de la connaissance sont réalisées avec une grammaire et un vocabulaire restreints, afin de réduire l'ambiguïté, et donc la complexité des textes. Un tel langage permet à un programme informatique de comprendre une phrase. Ces  notions sont comprises dans l'expression "langage contrôlé". Ainsi cet ensemble de mots peut être utilisé pour vérifier la cohérence du texte ou encore de permettre à un utilisateur d'effectuer des interrogations sur l'ensemble des connaissances décrites.

\begin{shadedfigure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/dag_go_relation.png}
    \caption{Exemple de représentation de concepts et de leurs relations selon \acrfull{GO}.\hspace{\textwidth}\tiny{Source: \url{ftp://ftp.geneontology.org/pub/go/www/GO.ontology.relations.shtml}.}}
    \label{fig:go_relation}
\end{shadedfigure}


La structure des données de \gls{GO} répond à nos besoins. Cependant, les liens entre les termes d'une catégorie avec une autre catégorie ne sont pas fournis. Par exemple, on ne peut pas relier des processus biologiques à des fonctions moléculaires. Des projets comme \cite{AdditionalGO2006} parviennent partiellement à couvrir les liens  entre les termes des différentes catégories. Toutefois, le nombre de relations manquantes reste élevé. Pour cette raison, j'ai mis au point une structure de concepts contenant toutes les notions nécessaires pour représenter les connaissances à utiliser. Cette structure doit devenir le support d'inférence des observations biologiques, l'objectif étant de vérifier l'existentialité de nos connaissances sur chaque organisme.

Tout au long de mon travail de recherche, j'ai été confronté à des limites, si bien que des solutions nouvelles devaient être proposées. Des problématiques d'apparence simple se sont révélées plus complexes. J'ai ainsi dû rechercher des solutions dans des domaines "éloignés" de la bio-informatique comme \textit{la Logique}, afin de définir un modèle générique représentant toute sorte de données, même celles auxquelles je n'avais pas pensées ! J'ai également mis en place des méthodes permettant d'intégrer un volume de données conséquent tout en étant capable de fournir un résultat dans un temps raisonnable. Ce travail m'a demandé de dépasser les paradoxes de la logique afin de l'étendre aux notions d'"inconnu" et de "contradiction" ; la méthode finale doit proposer des annotations manquantes, mettre en lumière des contradictions, vérifier les prédictions \textit{in silico} avec les "expectations" biologiques (voir Note ci-dessous), et explorer les capacités métaboliques de tout organisme.

\note{Ce travail de recherche s'attache à prendre en compte les fonctions biologiques attendues dans un organisme. Ce savoir peut provenir de fait établis, de suppositions ou autres. Lorsque nous sommes confronté à de tels faits, nous espérons trouver une théorie qui corrèle les faits observés. Pour exprimer cette idée, je vais utiliser au long de cette thèse le terme expectation. C'est un mot transparent. Il peut être utilisé en français et en anglais. Il vient du latin \textit{exspectatio} \cite[p. 637]{gaffiot1934dictionnaire} et permet d'exprimer une anticipation.\\\tiny{sources:\\ \url{http://www.larousse.fr/dictionnaires/francais/expectation/32223}\\\url{https://fr.wiktionary.org/wiki/exspectatio} } }

Dans la suite de cette thèse, je vais tout d’abord expliciter les concepts métaboliques et leurs représentations informatiques. Puis, les liens entre génome et métabolisme seront détaillés, avant de s'intéresser aux données biologiques qui sont à notre disposition pour l’analyse de fonctions telles que les réseaux métaboliques. La section suivante abordera des notions de \textit{Logique} ainsi que des méthodes existantes aujourd'hui pour résoudre des problèmes biologiques. Je présenterai ensuite mes investigations sur la logique paracohérente et les premières phases du développement d'un système expert nommé \gls{GROOLS}. Le dernier chapitre porte sur le raisonnement descriptif finalement mis en œuvre dans la méthode \gls{GROOLS} et la présentation de résultats. Enfin, cette thèse se termine par des perspectives de recherche.



\end{refsegment}
