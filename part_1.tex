\chapter*{De l'information à la connaissance}
\markboth{De l'information à la connaissance}{De l'information à la connaissance}
\addcontentsline{toc}{chapter}{De l'information à la connaissance}

La recherche de la véracité des connaissances, à de tout temps était un enjeu majeur pour la science. Des méthodes scientifiques ont été conçue afin d'être rigoureuses et vérifiable sur les résultats. Toutefois avec un nombre sans cesses grandissants de publication scientifique, estimé à plus 50 millions \citep[voir][]{LEAP:LEAP0509}. On peu légitimement se demander:
\begin{itemize}
	\item "Ces connaissances sont-elles toutes vrai ?"
	\item "Est-ce encore de la connaissance ou de l'information ?"
	\item "Peut-on généraliser une connaissance ?"
\end{itemize}

D'autre part à l'origine des connaissances scientifiques, il y a des observations permettant d'émettre des prédictions. Et cette décennie à vue l'émergence des technologies de l'information et les problématiques liées au "Big Data". Notre capacité à générer des informations est sans cessent grandissante. Elle dépasse largement les capacités humaines nécessaire à l'expertise de ces informations. En effet, les outils de mesures génèrent un nombre incalculable d'observation en temps réelle. Ces diverses observations brutes sont ensuite utilisées par des méthodes informatisés afin d'émettre des prédictions issues d'observation primaire.

Les super-calculateurs, support de ces méthodes informatisées, deviennent de plus en plus rapide, augmentant les capacités de traitement de l'information.  De tels outils, se démocratisent dans tous les domaines de la science. Pour autant seul une partie de ces informations peuvent être étudié afin de créer de nouvelles connaissances. Avec l'essor des nouvelles technologies, la barrière entre information et connaissance devient de plus en plus flou.

\note{Il est intéressant de remarquer que le mot science vient du latin "scientia" désignant la connaissance.}

Ces données produites par la recherche sont entreposées dans des bases de données variées, adaptés aux différentes spécialités. Certains entrepôts de données peuvent couvrir un même champ de connaissance. Ne disposant pas des mêmes données, tantôt en la complétant, tantôt en la contredisant. Laissant même des régions de connaissances couverte par aucune donnée, appelé "trou de connaissance ".

Il est indubitable que  nous ne pouvons plus mener des recherches sur des vastes domaines comme "la Biologie", "les Mathématiques", "la Physique". Les connaissances à maitriser sont trop nombreuses. Les chercheurs, se spécialisent de plus en plus afin de maitriser un domaine toujours plus pointus. Mais malgré tous ces efforts, dans de nombreux domaines, il nous est impossible d'appréhender tous les travaux liés. Or ces connaissances, nous sont non seulement
importantes pour valider ou non les prédictions; mais il nous est également nécessaire d'élargir nos connaissances, hors de notre domaine d'expertise pour mieux le comprendre en retour.

Rechercher la véracité des connaissances, nécessite de pouvoir représenter nos supposés connaissances puis de les confrontés aux différentes prédictions et expectations. Ce travail ne peut plus être réalisé uniquement par l'Homme, il doit être assisté par des méthodes informatisés capable de répondre aux défis d'aujourd'hui et de demain.

Ce constat se vérifie en biologie, avec l'avènement des séquenceurs nouvelles génération. Le séquençage des organismes est devenu abordable et rapide. Ouvrant la voie à de très nombreux projets de recherche. Nous permettant d'avoir rapidement le code génétiques de tout organisme, également nommé séquence \gls{ADN}. Cette séquences peut faire quelque centaines de millier à plusieurs centaines de million de paire de bases nucléiques.

Des outils bio-informatiques analysent ces séquences afin de prédire les régions géniques et leur fonction dans l'organisme. Le nombre de gène peut aller de quelques centaines à plusieurs dizaine de millier selon les organismes. Par conséquent l'expertise humaine d'un génome et un défi en soi. Ces recherches se sont intensifiées et complexifiées, comme l'étude de 100 000 génomes d'organismes pathogènes \citep[voir][]{100kfoodborne}, ou encore sur les eco-systèmes poly-microbiens présent sur l'Homme \citep[voir][]{hmp}.

Pour se faire des outils bio-informatiques ont automatisé le traitement de l'information issue des séquenceurs afin de traiter un nombre d'organisme toujours plus grand. Ces outils, alimentés continuellement en nouveaux génomes, ont amplifié le déluge d'information. Dans le domaine de l'annotation fonctionnelle, moins d'un pour cent des données ont put être vérifiés (au regards des statistiques publiés par uniprot et swissprot \parencites{uniprot_stat}{expasy_stat} ). Ce fossé entre information de confiance et prédiction s'accélère.

Or les prédicteurs automatiques de fonctions génique ne sont pas fiables. En effet, 30\% des annotations fonctionnelles seraient incorrectes, voire 80\% dans certaines familles de protéines \parencites{devos2001intrinsic}{schnoes2009annotation}. Ces séquences incorrectement annotées sont ensuite propagées dans les bases de connaissances.

Une des méthodes d'assignation de fonction à un gène, consiste de rechercher une séquence proches, dont la fonction est connue. Supposant qu'ils partagent une même fonction. Ainsi on inférera l'annotation, à la séquence de fonction inconnue, car elle est proche de la première. Cette pratique tant a amplifié l'inexactitude des fonctions géniques. En effet, les séquences mal annotées se retrouvent parmi les autres et sont donc potentiellement ré-utilisées \unsure{est-ce utile de remettre une couche?} afin d'inférer une fonction un autre gène considérait proche. Détériorant un peu plus la qualité de ces base de données.

L'objectif de l'annotation des fonctions géniques et de fournir un catalogue des capacités moléculaires et/ou biochimiques dont est pourvu un organisme, et donc a posteriori de mieux comprendre le vivant. Mais lorsque le processus d'annotation, produit et amplifie l'assignation de mauvaise fonctions aux gènes. Cela a pour effet de ne plus pouvoir utiliser ces prédictions sans prendre un risque. De plus, ce catalogue de fonctions géniques est utilisé par la suite dans de nombreux domaines, comme l'étude des voies métaboliques, la biologie des systèmes, la classification des gènes essentiels et autres. Cette problématique impacte notre compréhension du vivant et notre capacité à l'étudier, remettant en cause tous le processus d'annotation des gènes utilisé jusqu'alors.

Face à cette problématique des approches variées ont été développées. On distingue les systèmes d'annotations automatiques à base de règle, reprenant le raisonnement appliqué par les bio-curateurs, comme le projet HAMAP \citep[voir][]{lima2009hamap}. Ces règles sont généralement basé sur la séquence génomique et la taxonomie de l'organisme. Elle peuvent être crée par un bio-curateur ou par des outils d'apprentissage \citep[voir][]{uniprot2011ongoing}.

On retrouve également les systèmes de reconstruction des voies métaboliques \citep[voir][]{karpe2011pathway}, utilisant les prédictions de fonction génique, spécifiques d'un organisme afin de proposé des voies métaboliques décrites dans d'autres organismes. Ces prédictions forment un graphe de connaissance, spécifique de l'organisme. Ainsi, des fonctions marquées comme manquante à la réalisation des voies métaboliques sont suggérées aux bio-curateurs.

Les méthodes à bases de règles automatise l'annotation fonctionnelle par l'utilisation de règle lié à la biologie de l'organisme et non plus uniquement par une prédiction in-silico.  Les méthodes utilisant la représentations des connaissances biologiques permettent de contextualiser les prédictions et de suggérer des annotations fonctionnelles ne pouvant être détecté in-silico. Toutefois le travail de curations des annotations par un bio-curateur est nécessaire, mais considéré comme fastidieux, laborieux et source d'erreur. Il est nécessaire de fournir un assistant à la curation des fonctions géniques.

%L'équipe HELIX dirigé par Alain Viari (INRIA) à développé un prototype de vérification de la cohérence globale de l'annotation (HERBS). Reprenant les systèmes à base de règle et la représentation des connaissances. Ainsi les voies métaboliques peuvent être rattaché à des caractères phénotypique. Par exemple, la pousse d'un organisme sur un milieu avec une seule source de carbone, un antibiotique \ldots peuvent être relié au graphe de connaissance et ainsi comparé les prédictions par rapports aux attentes issues des données expérimentales. 


%Cet discipline consiste à caractériser la fonction des gènes, afin de lister les fonctionnalités dont est pourvu un organisme. En amont de ce travail les outils bio-informatiques émettent des prédictions sur les fonctions des gènes identifiés. Toutefois seulement une partie de ces prédictions sont vérifiées expérimentalement ou par un curateur. Or de nombreuses études montre les limites de l'annotation fonctionnelle automatiques. 

\section*{La démarche suivie dans cette thèse}

\chapter{Contexte biologique}

\section{Représentation des connaissances}
\subsection{Représentation des voies métaboliques}
\subsubsection{KEGG}
\subsubsection{Reactome}
\subsubsection{Unipathway}
\subsubsection{Genome properties}
\subsection{Représentation des résultats expérimentaux}
néant

\section{Nature de l'information biologique}
\subsection{Observation}
\subsection{Prédiction}
\subsection{Expectation}

\section{De l'annotation géniques aux métabolismes}
\subsection{Annotations fonctionnelles des génomes}
\subsection{Reconstruction de réseaux et modèles métaboliques}
\section{Les analyses bioinformatiques}
\section{Les expérimentations en laboratoire}

\section{Des résultats à l'interprétation automatique}
\subsection{La biologie un monde d'incertitude}
\subsection{Pluralité de l'information}
diversité des résultats contradiction
\subsection{La logique classique}
\subsection{La logique multi-valuée}
belnap and co
\subsection{HERBS}
