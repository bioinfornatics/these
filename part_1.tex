\chapter*{De l'information à la connaissance}
\markboth{Introduction --- De l'information à la connaissance}{De l'information à la connaissance}
\addcontentsline{toc}{chapter}{Introduction --- De l'information à la connaissance}

Un des enjeux majeur de la science, a été de tout temps, la recherche de la véracité des connaissances. En effet, une connaissance communément acceptée, permet d'aboutir à un savoir transmissible. Pour cela, il est indispensable de procéder avec méthode.

Dès l'antiquité, la nécessité de démontrer, les causes expliquant avec certitudes les conséquences, émerge avec Aristote (384 - 322 av. J.-C.). À travers Alhazen (965 - 1039) l'idée pris forme autour de méthodes expérimentales et reproductibles. Son traité \citetitle{Alhazen1572} détaille des expériences reproductibles, ainsi tout lecteur, parvient aux mêmes conclusions. Ses méthodes furent reprises en Occident par Roger Baconn (1220 - 1294). Cette démarche expérimentale décrit les prémisses d'une méthode scientifique. Elle a évolué tout au long de l'histoire, afin d'être rigoureuse, vérifiable et reproductible.

Ainsi, le trio observation, modélisation et expérimentation s'est imposé comme fondement d'une démarche scientifique. Ces trois paramètres permettent d'amener les éléments de confiance et d'impartialité nécessaire pour vérifier une connaissance.

Cependant l'explosion des connaissances a rendu cette démarche fastidieuse. En effet, elle nécessite une intervention humaine minutieuse de toutes les observations vis-à-vis du savoir. Par exemple le nombre de communications scientifique, est estimé à plus 50 millions \citep[voir][]{LEAP:LEAP0509}. C'est autant de connaissances à confronter  On peut légitimement se demander, s'il est possible de confronter l'ensemble des observations, vis-à-vis d'un ensemble de connaissance. Ceci afin d'évaluer nos certitudes et produire in-fine du savoir.

\citation{
	Nous estimons posséder la science d'une chose d'une manière absolue, \ldots quand nous croyons que nous connaissons la cause par laquelle la chose est, que nous savons que cette cause est celle de la chose, et qu'en outre il n'est pas possible que la chose soit autre qu'elle n'est.}{Aristote}[(Seconds Analytiques I, 31, 88a, 4)]

Les observations sont des éléments précurseurs à la connaissance, elles sont utilisés pour émettre des théories. L'émergence des technologies de l'information nous permettent de récupérer un nombre d'observations toujours plus grand. Cet évènement ouvre de nouvelles possibilité comme l'étude massive des observations scientifiques. C'est l'ère du "Big Data". Notre capacité à générer des informations est sans cesse grandissante. Elle dépasse largement les capacités humaines nécessaires à l'expertise de ces informations. En effet, les outils de mesure génèrent continuellement et rapidement des observations. Ces diverses observations brutes sont ensuite utilisées par des méthodes informatisées afin d'émettre des prédictions issues de ces observations primaires. A développer? ou veut on en arriver?

Les super-calculateurs, support de ces méthodes informatisées, deviennent de plus en plus rapide, augmentant les capacités de traitement de l'information. De tels outils, se démocratisent dans tous les domaines de la science. Pour autant seul une partie de ces informations peuvent être étudiée afin de créer de nouvelles connaissances. Avec l'essor des nouvelles technologies, la barrière entre information et connaissance devient de plus en plus flou.

\note{Il est intéressant de remarquer que le mot science vient du latin \textit{"scientia"} désignant la connaissance.}

Ces données produites par la recherche sont entreposées dans des bases de données variées, adaptées aux différentes spécialités. Certains entrepôts de données peuvent couvrir un même champs de connaissance. Ne disposant pas des mêmes données, tantôt en la complétant, tantôt en la contredisant, laissant même des régions de connaissances couverte par aucune donnée, appelé "trou de connaissance ".

Il est indubitable que nous ne pouvons plus mener des recherches sur des vastes domaines comme "la Biologie", "les Mathématiques", "la Physique". Les connaissances à maitriser sont trop nombreuses. Les chercheurs, se spécialisent de plus en plus afin de maitriser un domaine toujours plus pointu. Malgré tous ces efforts, dans de nombreux domaines, il nous est impossible d'appréhender tous les travaux liés. Or ces connaissances, nous sont non seulement
importantes pour valider ou non les prédictions ; mais il nous est également nécessaire d'élargir nos connaissances, hors de notre domaine d'expertise pour mieux le comprendre en retour.(trop long)

Rechercher la véracité des connaissances, nécessite de pouvoir représenter nos supposées connaissances puis de les confrontés aux différentes prédictions et expectations. Ce travail ne peut plus être réalisé uniquement par l'Homme, il doit être assisté par des méthodes informatisées capable de répondre aux défis d'aujourd'hui et de demain.

Ce constat se vérifie en biologie, avec l'avènement des séquenceurs nouvelles générations. Le séquençage des organismes est devenu abordable et rapide. Ainsi la communauté scientifique à initier de vaste projet de séquençage du monde vivant. Ces projets permettent d'avoir le code génétique, également nommée séquence \gls{ADN}.L'ADN peut faire quelques centaines de milliers à plusieurs centaines de millions de paires de bases nucléiques. Cette séquence nucléotidique est un point départ pour l'étude et la compréhension des fonctionnalités inscrite dans le vivant.

Des outils bio-informatiques analysent ces séquences afin de prédire les régions géniques et leurs fonctions dans l'organisme. Le nombre de gènes peut aller de quelques centaines à plusieurs dizaines de milliers selon les organismes. Par conséquent, l'expertise humaine d'un génome est un défi en soi. Ces recherches se sont intensifiées et complexifiées, comme l'étude de 100 000 génomes d'organismes pathogènes \citep[voir][]{100kfoodborne}, ou encore sur les eco-systèmes poly-microbiens présent sur l'Homme \citep[voir][]{hmp}.

Pour se faire des outils bio-informatiques ont automatisé le traitement de l'information issue des séquenceurs afin de traiter un nombre d'organisme toujours plus grand. Ces outils, alimentés continuellement en nouveaux génomes, ont amplifié le déluge d'information. Dans le domaine de l'annotation fonctionnelle, moins d'un pour cent des données ont pu être vérifiées (au regard des statistiques publiés par UniProt et SwissProt \parencites{uniprot_stat}{expasy_stat} ). Ce fossé entre information de confiance et prédiction s'accélère car il est plus rapide de produire de l'information que de la vérifier.

Or les prédicteurs automatiques de fonctions géniques ne sont pas fiables. En effet, 30\% des annotations fonctionnelles seraient incorrectes, voire 80\% dans certaines familles de protéines \parencites{devos2001intrinsic}{schnoes2009annotation}. Ces séquences incorrectement annotées sont ensuite propagées dans les bases de connaissances.

Une des méthodes d'assignations de fonction a un gène, consiste à rechercher une séquence proche, dont la fonction est connue. Supposant qu'ils partagent une même fonction.(a reformulé) Ainsi on inférera l'annotation, à la séquence de fonction inconnue, car elle est proche de la première. Cette pratique tend à amplifier l'inexactitude des fonctions géniques. En effet, les séquences mal annotées se retrouvent parmi les autres et sont donc potentiellement ré-utilisées \unsure{est-ce utile de remettre une couche?} afin de propager une fonction d'un autre gène considéré proche. Détériorant un peu plus la qualité de ces bases de données.

L'objectif de l'annotation des fonctions géniques est de fournir un catalogue des capacités moléculaires et/ou biochimiques dont est pourvu un organisme. Ce catalogue permet de mieux comprendre le vivant.( a reformulé) Mais lorsque le processus d'annotation, produit et amplifie l'assignation de mauvaise fonction aux gènes. Cela entraine l'incapacité à utiliser ces prédictions sans prendre un risque. De plus, ce catalogue de fonctions géniques est utilisé par la suite dans de nombreux domaines, comme l'étude des voies métaboliques, la biologie des systèmes, la classification des gènes essentiels et autres. Cette problématique impacte notre compréhension du vivant et notre capacité à l'étudier, remettant en cause tout le processus d'annotation des gènes utilisés jusqu'alors.

Face à cette problématique des approches variées ont été développées. On distingue les systèmes d'annotations automatiques à base de règle, reprenant le raisonnement appliqué par les bio-curateurs, comme le projet HAMAP \citep[voir][]{lima2009hamap}. Ces règles sont généralement basé sur la séquence génomique et la taxonomie de l'organisme. Elles peuvent être créées par un bio-curateur ou par des outils d'apprentissage \citep[voir][]{uniprot2011ongoing}.

On retrouve également les systèmes de reconstruction des voies métaboliques \citep[voir][]{karpe2011pathway}, utilisant les prédictions de fonction génique, spécifiques d'un organisme afin de proposé des voies métaboliques décrites dans d'autres organismes. Ces prédictions forment un graphe de connaissance, spécifique de l'organisme. Ainsi, des fonctions marquées comme manquante à la réalisation des voies métaboliques sont suggérées aux bio-curateurs.

Les méthodes à bases de règles automatise l'annotation fonctionnelle par l'utilisation de règle lié à la biologie de l'organisme et non plus uniquement par une prédiction in-silico. Les méthodes utilisant la représentations des connaissances biologiques permettent de contextualiser les prédictions et de suggérer des annotations fonctionnelles ne pouvant être détecté in-silico. Toutefois le travail de curations des annotations par un bio-curateur est nécessaire, mais considéré comme fastidieux, laborieux et source d'erreur. Il est nécessaire de fournir un assistant à la curation des fonctions géniques.

%L'équipe HELIX dirigé par Alain Viari (INRIA) à développé un prototype de vérification de la cohérence globale de l'annotation (HERBS). Reprenant les systèmes à base de règle et la représentation des connaissances. Ainsi les voies métaboliques peuvent être rattaché à des caractères phénotypique. Par exemple, la pousse d'un organisme sur un milieu avec une seule source de carbone, un antibiotique \ldots peuvent être relié au graphe de connaissance et ainsi comparé les prédictions par rapports aux attentes issues des données expérimentales. 

%Cet discipline consiste à caractériser la fonction des gènes, afin de lister les fonctionnalités dont est pourvu un organisme. En amont de ce travail les outils bio-informatiques émettent des prédictions sur les fonctions des gènes identifiés. Toutefois seulement une partie de ces prédictions sont vérifiées expérimentalement ou par un curateur. Or de nombreuses études montre les limites de l'annotation fonctionnelle automatiques. 

\section*{La démarche suivie dans cette thèse}
\begin{refsection}
\chapter{Contexte biologique et méthodologique }

\section{Le métabolisme et sa représentation informatique}
\subsection{Généralités sur le métabolisme}
\subsection{Les acteurs}
\subsection{Représentation en graphe}
\subsection{Ressources sur les voies métaboliques}
\subsubsection{KEGG}
\subsubsection{Reactome}
\subsubsection{Unipathway}
\subsubsection{Genome properties}

\section{Des génomes aux réseaux métaboliques}
\subsection{Annotation fonctionnelle}
\subsection{Reconstruction des réseaux métaboliques}
\subsection{Modèles métaboliques}
\subsection{Les données expérimentales}
\subsubsection{Élucidation des voies métaboliques}
\subsubsection{Phénotypes de croissances}

\section{Raisonnement logique dans le processus de curation}
\subsection{Lacunes et incertitudes dans nos connaissances}
\subsubsection{Les trous dans les connaissances et les enzymes orphelines}
\subsubsection{Limites de l’annotation fonctionnelle et rôle de la curation}
\subsection{Logique et raisonnement}
\subsubsection{Les différentes logiques}
\paragraph{Logique booléenne}
\paragraph{Logique multi-valuée}
\subsubsection{Inférence d’information}
\paragraph{Représenation des connaissances/ontologies}
\paragraph{Chainage avant et arrière} %backward/forward
\paragraph{Règles et système expert}

\section{Méthodes existantes}
\subsection{HAMAP et UniRule}
\subsection{Genome properties}
\subsection{IMG terms}
\subsection{HERBS} \citep[voir][]{lima2009hamap}
\printbibliography[segment=\therefsegment,heading=subbibliography]
\end{refsection}